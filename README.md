# Multi-Tenant AI Chatbot Platform

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](./tests)
[![Test PyPI](https://img.shields.io/badge/Test%20PyPI-v1.0.0-3775A9)](https://test.pypi.org/project/chatbot-ai-system/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

## Overview

Enterprise-grade conversational AI platform delivering production-ready multi-tenant chatbot capabilities with provider-aware orchestration (OpenAI, Anthropic), real-time WebSocket streaming, semantic caching, and full observability (Prometheus, Grafana, Jaeger). Built for reliability, scalability, and cost efficiency, with reproducible benchmarks and a minimal Next.js UI.

---

## Performance Metrics

### Verified Benchmarks

Results and reports live under `benchmarks/results/` and are generated by the included scripts.

| Metric              |   Target |              Achieved | Evidence                                                             |
| ------------------- | -------: | --------------------: | -------------------------------------------------------------------- |
| Latency P95         | < 200 ms |            **187 ms** | `benchmarks/results/rest_api_latest.json` and `rest_api_latest.html` |
| Latency P99         | < 300 ms |            **245 ms** | `benchmarks/results/rest_api_latest.json`                            |
| Requests/sec        | 400+ RPS | **\~450 RPS** (local) | `benchmarks/results/rest_api_latest.json`                            |
| Cache hit rate      |   ≥ 0.60 |              **0.73** | `benchmarks/results/cache_metrics_latest.json`                       |
| Est. cost reduction |    ≥ 30% |             **31.2%** | `benchmarks/results/cache_metrics_latest.json`                       |
| Failover time       | < 500 ms |          **\~487 ms** | `benchmarks/results/failover_timing_latest.json`                     |

### Reproduction Steps

```bash
# Prerequisites: Docker (and k6 if you run load tests locally)
git clone https://github.com/cbratkovics/ai-chatbot-system.git
cd ai-chatbot-system

# End-to-end demo (build, test, benchmark)
make demo

# Or run individual steps
make demo-up         # Start API + Redis
make demo-test       # Failover verification
make demo-benchmark  # k6 API tests
make demo-clean      # Tear down

# View results
open benchmarks/results/rest_api_latest.html
```

---

## Architecture

### System Design

```mermaid
flowchart LR
  subgraph Client
    UI[Next.js UI] --- WS[WebSocket]
    CLI[HTTP Client]
  end

  WS -->|stream| API[(FastAPI ASGI)]
  CLI --> API

  subgraph API Layer
    API --> MW[Middleware: Tenant • RateLimit • Tracing]
    MW --> Orchestrator[Provider Orchestrator\n(circuit breaker + retry)]
    MW --> Cache[Semantic Cache (Embeddings + Redis)]
  end

  Orchestrator -->|primary| OpenAI[(OpenAI Provider)]
  Orchestrator -->|failover| Anthropic[(Anthropic Provider)]

  Cache <--> Redis[(Redis)]

  subgraph Observability
    API --> Prom[Prometheus Metrics]
    API --> Jaeger[Jaeger Tracing]
    Prom --> Grafana[Grafana Dashboards]
  end
```

---

# AI Chatbot Orchestration Platform

A production-ready Python package for orchestrating multiple AI providers (OpenAI, Anthropic) with caching, WebSocket streaming, rate limiting, and robust error handling.

## Key Features

* **Multi-provider orchestration and failover**
  Strategy selection, circuit breaker, and retries across OpenAI and Anthropic.

* **Real-time streaming**
  WebSocket endpoint for token-by-token streaming.

* **Semantic caching for cost reduction**
  Redis-backed cache with embeddings and similarity thresholds; hit-rate and savings tracked.

* **Multi-tenancy and controls**
  Per-tenant context, token-bucket rate limiting, and auth hooks.

* **Observability**
  Prometheus metrics, Grafana dashboards, and Jaeger traces.

* **Developer experience**
  `src/` package layout, Poetry, Makefile tasks, Docker Compose, and a minimal Next.js frontend.

---

## Installation

Install from Test PyPI (uses PyPI as a fallback index for dependencies):

```bash
python -m pip install --upgrade pip

python -m pip install \
  --index-url https://test.pypi.org/simple/ \
  --extra-index-url https://pypi.org/simple \
  chatbot-ai-system==1.0.0
```

Run the ASGI app from the installed package:

```bash
uvicorn chatbot_ai_system.server.main:app --port 8000
# http://localhost:8000/docs
```

---

## Quick Start

### Option A — Docker

```bash
docker compose up -d
# API: http://localhost:8000/docs
# Health: http://localhost:8000/health

# Frontend (optional)
cd frontend
npm install
npm run dev
# http://localhost:3000
```

### Option B — Local Dev (Poetry)

```bash
poetry install
poetry run pytest -q
poetry run uvicorn chatbot_ai_system.server.main:app --reload --port 8000
```

Environment: copy `.env.example` → `.env` and set provider keys, Redis URL, rate limits, request timeout, and default models.

---

## API Examples

### Health

```bash
curl http://localhost:8000/health
```

### Chat Completion (REST)

```bash
curl -X POST http://localhost:8000/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "default",
        "messages": [{"role": "user", "content": "Hello!"}],
        "stream": false
      }'
```

### Streaming (WebSocket)

Connect to `ws://localhost:8000/api/v1/ws` (the Next.js UI does this automatically).

---

## Project Structure

```
.
├─ src/chatbot_ai_system/
│  ├─ server/               # ASGI app & routing
│  ├─ providers/            # OpenAI, Anthropic, base provider, strategies
│  ├─ middleware/           # tenant, rate limit, tracing
│  ├─ cache/                # semantic cache (embeddings + Redis)
│  ├─ config/               # settings & env management
│  └─ health.py             # health/readiness endpoints
├─ benchmarks/              # k6, Locust, Python harness, results
├─ monitoring/              # Prometheus, Grafana, Jaeger configs
├─ frontend/                # Next.js streaming UI
├─ tests/                   # unit/integration tests
├─ docker-compose.yml
├─ Makefile
└─ pyproject.toml
```

---

## Testing

```bash
# All tests
poetry run pytest -v

# Unit only
poetry run pytest tests/unit -v

# Integration only
poetry run pytest tests/integration -v

# Coverage report (if configured)
poetry run pytest --cov=chatbot_ai_system --cov-report=html
```

---

## Docker Support

```bash
# Build and run
docker build -t chatbot-ai-system .
docker run -p 8000:8000 chatbot-ai-system

# Or compose
docker compose up -d
```

---

## Configuration

### Backend Configuration

Environment variables (see `.env.example`):

```
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...
REDIS_URL=redis://localhost:6379/0
LOG_LEVEL=INFO
RATE_LIMIT_TOKENS=...
DEFAULT_MODEL=...
REQUEST_TIMEOUT_SECONDS=...
```

### Frontend Configuration

The frontend Next.js application requires environment variables (see `frontend/.env.example`):

```bash
# Copy example configuration
cd frontend
cp .env.example .env.local

# Key variables:
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000
NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
NEXT_PUBLIC_WS_URL=ws://localhost:8000/ws/chat
```

Start the frontend:

```bash
cd frontend
npm install
npm run dev
# Open http://localhost:3000
```

---

## Performance Notes

* p95 latency under 200 ms and p99 under 300 ms in local benchmarks.
* \~450 RPS achieved in local API tests using k6.
* Semantic cache hit rate \~0.73 with estimated cost reduction \~31.2%.
* Failover completes in \~487 ms in isolated tests.

See the full reports under `benchmarks/results/`.

---

## Security

See `SECURITY.md` for reporting guidelines.

---

## License

MIT © Christopher J. Bratkovics

---

## Acknowledgments

FastAPI, Uvicorn, Redis, Prometheus, Grafana, and Jaeger. Providers currently wired: OpenAI, Anthropic.
